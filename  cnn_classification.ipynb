{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matrices loaded: 2611\n",
      "Total labels loaded: 2611\n",
      "Class weights: {0: 27.197916666666668, 1: 0.7118320610687023, 2: 0.40243526510480887, 3: 13.598958333333334}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "def load_matrix(data, matrix_size=200):\n",
    "    # Filter interactions within the same chromosome\n",
    "    data = data[data['chr1'] == data['chr2']]\n",
    "    # Determine the range of positions\n",
    "    max_pos = data[['pos1', 'pos2']].max().max()\n",
    "    min_pos = data[['pos1', 'pos2']].min().min()\n",
    "    # Define the bin size dynamically based on the desired matrix size\n",
    "    bin_size = (max_pos - min_pos) / matrix_size\n",
    "    # Create bins\n",
    "    bins = np.linspace(min_pos, max_pos, matrix_size + 1)\n",
    "    # Create a mapping of positions to bins\n",
    "    data['bin1'] = np.digitize(data['pos1'], bins) - 1\n",
    "    data['bin2'] = np.digitize(data['pos2'], bins) - 1\n",
    "    # Ensure bins do not exceed the matrix size\n",
    "    data['bin1'] = data['bin1'].clip(upper=matrix_size - 1)\n",
    "    data['bin2'] = data['bin2'].clip(upper=matrix_size - 1)\n",
    "    # Create unique identifiers for each bin\n",
    "    data['bin_id1'] = data['chr1'].astype(str) + '_' + data['bin1'].astype(str)\n",
    "    data['bin_id2'] = data['chr2'].astype(str) + '_' + data['bin2'].astype(str)\n",
    "    # Get unique bins\n",
    "    unique_bins = np.unique(np.concatenate([data['bin_id1'], data['bin_id2']]))\n",
    "    # Initialize the contact matrix\n",
    "    contact_matrix = np.zeros((matrix_size, matrix_size))\n",
    "    # Populate the contact matrix\n",
    "    for _, row in data.iterrows():\n",
    "        idx1 = row['bin1']\n",
    "        idx2 = row['bin2']\n",
    "        contact_matrix[idx1, idx2] += row['interaction']\n",
    "        contact_matrix[idx2, idx1] += row['interaction']  # Assuming symmetry\n",
    "    \n",
    "    return contact_matrix\n",
    "\n",
    "def find_max_shape_matrix(genotype: str) -> str:\n",
    "    addr = \"\"\n",
    "    max = (0, 0)\n",
    "    for file in os.listdir(genotype):\n",
    "        filepath = os.path.join(genotype, file)\n",
    "        full = pd.read_csv(filepath, sep='\\t', header=None)\n",
    "        full.columns = ['chr1', 'pos1', 'chr2', 'pos2', 'interaction']\n",
    "        # Find unique values in column 2 (pos1) and column 4 (pos2)\n",
    "        full_posx = full['pos1'].unique()\n",
    "        full_posy = full['pos2'].unique()\n",
    "        if len(full_posx) > max[0] and len(full_posy) > max[1]:\n",
    "            max = (len(full_posx), len(full_posy))\n",
    "            addr = filepath\n",
    "\n",
    "    return addr\n",
    "\n",
    "def augment(genotype:str, filepaths:list[str]) -> list[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    genotype: str, the genotype directory, eg. \"./GM12878\"\n",
    "    filepaths: list[str], the list of filepaths of the matrices in the genotype directory\n",
    "    \"\"\"\n",
    "    augmented_matrices = [] # Stores all augmented matrices\n",
    "\n",
    "    max_size_file_path = find_max_shape_matrix(genotype)\n",
    "\n",
    "    # Find the reference axes\n",
    "    reference_data = pd.read_csv(max_size_file_path, sep='\\t', header=None)\n",
    "    # Assign column names\n",
    "    reference_data.columns = ['chr1', 'pos1', 'chr2', 'pos2', 'interaction']\n",
    "    # Find unique values in column 2 (pos1) and column 4 (pos2)\n",
    "    unique_posx = reference_data['pos1'].unique()\n",
    "    unique_posy = reference_data['pos2'].unique()\n",
    "\n",
    "    # Find all (200, 200) full matrices\n",
    "    full_matrices_dir = []\n",
    "    for file in os.listdir(genotype):\n",
    "        filepath = os.path.join(genotype, file)\n",
    "        if os.path.isfile(filepath):\n",
    "            matrix = load_matrix(pd.read_csv(filepath, sep='\\t', header=None, names=['chr1', 'pos1', 'chr2', 'pos2', 'interaction']))\n",
    "            if matrix.shape == (200, 200):\n",
    "                full_matrices_dir.append(filepath)\n",
    "                augmented_matrices.append(matrix)\n",
    "\n",
    "    # Find all matrices that are not 200x200\n",
    "    not_full_matrices = []\n",
    "    for filepath in filepaths:\n",
    "        if filepath not in full_matrices_dir:\n",
    "            not_full_matrices.append(filepath)\n",
    "\n",
    "    for filepath in not_full_matrices:\n",
    "        temp_data = pd.read_csv(filepath, sep='\\t', header=None)\n",
    "        temp_data.columns = ['chr1', 'pos1', 'chr2', 'pos2', 'interaction']\n",
    "        temp_x = temp_data['pos1'].unique()\n",
    "        temp_y = temp_data['pos2'].unique()\n",
    "        matrix = load_matrix(pd.read_csv(filepath, sep='\\t', header=None, names=['chr1', 'pos1', 'chr2', 'pos2', 'interaction']))\n",
    "        diff_x = np.setdiff1d(unique_posx, temp_x)  # columns missing\n",
    "        diff_y = np.setdiff1d(unique_posy, temp_y)  # rows missing\n",
    "        missing_data = reference_data[reference_data['pos1'].isin(diff_x) | reference_data['pos2'].isin(diff_y)]\n",
    "        aug_data = pd.concat([temp_data, missing_data], ignore_index=True)\n",
    "        aug_matrix = load_matrix(aug_data)\n",
    "        augmented_matrices.append(aug_matrix)\n",
    "    return augmented_matrices\n",
    "\n",
    "# Function to load all matrices from a directory\n",
    "def load_matrices(directory, contact_matrices, filepaths):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            # matrix = load_contact_matrix(filepath)\n",
    "            matrix = load_matrix(pd.read_csv(filepath, sep='\\t', header=None, names=['chr1', 'pos1', 'chr2', 'pos2', 'interaction']))\n",
    "            contact_matrices.append(matrix)\n",
    "            filepaths.append(filepath)\n",
    "\n",
    "# Directories\n",
    "directories = {\n",
    "    'GM12878': './GM12878',\n",
    "    'HAP1': './HAP1',\n",
    "    'Hela': './Hela',\n",
    "    'K562': './K562'\n",
    "}\n",
    "\n",
    "# Load all matrices and labels\n",
    "contact_matrices = []\n",
    "labels = []\n",
    "\n",
    "for label, directory in directories.items():\n",
    "    load_matrices(directory, contact_matrices, labels, label)\n",
    "\n",
    "contact_matrices = np.array(contact_matrices)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Total matrices loaded: {contact_matrices.shape[0]}\")  # Debug print\n",
    "print(f\"Total labels loaded: {labels.shape[0]}\")  # Debug print\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "labels_categorical = to_categorical(labels_encoded)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels_encoded), y=labels_encoded)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"Class weights:\", class_weights_dict)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(contact_matrices, labels_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape the data to add the channel dimension\n",
    "X_train = X_train.reshape(X_train.shape[0], 200, 200, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], 200, 200, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ImageDataGenerator with augmentation for the training data\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Use the generator to augment the training data\n",
    "train_generator = datagen.flow(X_train, y_train, batch_size=32)\n",
    "\n",
    "# Use a separate generator for the validation data without augmentation\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "validation_generator = validation_datagen.flow(X_val, y_val, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogersyang/Documents/PhD Application/Research/3D DNA Structure Heterogeneity (Elena)/DNA3D/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">198</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">147456</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">18,874,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m198\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_11 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_12 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m147456\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │    \u001b[38;5;34m18,874,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m516\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,893,828</span> (72.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,893,828\u001b[0m (72.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,893,828</span> (72.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,893,828\u001b[0m (72.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "# Define a simplified CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(200, 200, 1), kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogersyang/Documents/PhD Application/Research/3D DNA Structure Heterogeneity (Elena)/DNA3D/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 257ms/step - accuracy: 0.3520 - loss: 1.7005 - val_accuracy: 0.0000e+00 - val_loss: 1.4157 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 252ms/step - accuracy: 0.0473 - loss: 1.4330 - val_accuracy: 0.0000e+00 - val_loss: 1.4056 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 252ms/step - accuracy: 0.0098 - loss: 1.3981 - val_accuracy: 0.0000e+00 - val_loss: 1.4066 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 254ms/step - accuracy: 0.0082 - loss: 1.3620 - val_accuracy: 0.0000e+00 - val_loss: 1.4081 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 253ms/step - accuracy: 0.0112 - loss: 1.4292 - val_accuracy: 0.0000e+00 - val_loss: 1.4095 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 252ms/step - accuracy: 0.0149 - loss: 1.6338 - val_accuracy: 0.0000e+00 - val_loss: 1.4158 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 252ms/step - accuracy: 0.0097 - loss: 1.3996 - val_accuracy: 0.0000e+00 - val_loss: 1.4184 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 251ms/step - accuracy: 0.0124 - loss: 1.4691 - val_accuracy: 0.0000e+00 - val_loss: 1.4236 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 255ms/step - accuracy: 0.0127 - loss: 1.5157 - val_accuracy: 0.0000e+00 - val_loss: 1.4250 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 253ms/step - accuracy: 0.0103 - loss: 1.3155 - val_accuracy: 0.0000e+00 - val_loss: 1.4280 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 260ms/step - accuracy: 0.0067 - loss: 1.2781 - val_accuracy: 0.0000e+00 - val_loss: 1.4316 - learning_rate: 0.0010\n",
      "Epoch 12/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 265ms/step - accuracy: 0.0128 - loss: 1.5171 - val_accuracy: 0.0000e+00 - val_loss: 1.4341 - learning_rate: 0.0010\n",
      "Epoch 13/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 260ms/step - accuracy: 0.0117 - loss: 1.5232 - val_accuracy: 0.0000e+00 - val_loss: 1.4387 - learning_rate: 0.0010\n",
      "Epoch 14/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 261ms/step - accuracy: 0.0112 - loss: 1.4681 - val_accuracy: 0.0000e+00 - val_loss: 1.4404 - learning_rate: 0.0010\n",
      "Epoch 15/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 263ms/step - accuracy: 0.0111 - loss: 1.4088 - val_accuracy: 0.0000e+00 - val_loss: 1.4460 - learning_rate: 0.0010\n",
      "Epoch 16/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 250ms/step - accuracy: 0.0121 - loss: 1.4184 - val_accuracy: 0.0000e+00 - val_loss: 1.4416 - learning_rate: 0.0010\n",
      "Epoch 17/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 249ms/step - accuracy: 0.0099 - loss: 1.4722 - val_accuracy: 0.0000e+00 - val_loss: 1.4407 - learning_rate: 0.0010\n",
      "Epoch 18/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 259ms/step - accuracy: 0.0073 - loss: 1.2916 - val_accuracy: 0.0000e+00 - val_loss: 1.4430 - learning_rate: 0.0010\n",
      "Epoch 19/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 252ms/step - accuracy: 0.0064 - loss: 1.3429 - val_accuracy: 0.0000e+00 - val_loss: 1.4431 - learning_rate: 0.0010\n",
      "Epoch 20/20\n",
      "\u001b[1m66/66\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 254ms/step - accuracy: 0.0089 - loss: 1.3543 - val_accuracy: 0.0000e+00 - val_loss: 1.4493 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "\n",
    "# Train the model with class weights and learning rate scheduler\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=validation_generator,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.0000e+00 - loss: 1.4492\n",
      "Validation Accuracy: 0.00%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "The new matrix belongs to the collection: GM12878\n"
     ]
    }
   ],
   "source": [
    "# Initialize ImageDataGenerator with more aggressive augmentation for the training data\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Use the generator to augment the training data\n",
    "train_generator = datagen.flow(X_train, y_train, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "loss, accuracy = model.evaluate(validation_generator)\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Function to classify a new matrix\n",
    "def classify_matrix(matrix, model, label_encoder):\n",
    "    # Ensure the matrix is the correct shape\n",
    "    matrix = matrix.reshape(1, 200, 200, 1)  # Add batch and channel dimensions\n",
    "    matrix = matrix / 255.0  # Normalize\n",
    "    prediction = model.predict(matrix)\n",
    "    class_index = np.argmax(prediction, axis=1)\n",
    "    class_label = label_encoder.inverse_transform(class_index)\n",
    "    return class_label[0]\n",
    "\n",
    "# Example usage with a new matrix\n",
    "matrix = load_contact_matrix('./Hela/ml1_CACGACCT-CGTTACTT.txt')\n",
    "normalized_matrix = z_score_normalize(matrix)\n",
    "extended_matrix = extend_matrix(normalized_matrix)\n",
    "class_label = classify_matrix(extended_matrix, model, label_encoder)\n",
    "print(f'The new matrix belongs to the collection: {class_label}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
